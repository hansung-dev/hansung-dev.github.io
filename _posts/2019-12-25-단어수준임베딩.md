---
category: NLP
path: '/stuff/:id'
title: '단어 수준 임베딩'
type: 'NLP'

layout: nil
---

# 단어 수준 임베딩

1. 예측 기반 모델 : NPLM, Word2Vec, FastText
2. 행렬 분해 기반 기법 : LSA, CloVe, Swivel
3. 가중 임베딩 : 단어 임베딩을 문장 수준으로 확장하는 방법

***

## NPLM (Neural Probabilistic Language Model)

- '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생
- 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습된다.
- NPLM은 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델이다.
- 예시 : '발', '없는', '말이', ? < 예측 ('천리')

***

## Word2Vec

- 2013년 구글 연구 팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델이다.
- 타깃단어와 문맥단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플 인자, 네거티브 샘플인지 이진 분류하는 과정에서 학습한다.
- NPLM 보다 학습 파라미터 종류와 크기가 훨씬 작고 효율적인 학습이 가능하다.
- Skip-Gram, CBOW 그리고 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안
  - CBOW : 주변에 있는 문맥 단어 들을 가지고 타깃 단어 하나를 맞추는 과정
  - Skip-gram : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정 (네거티브 샘플링, 서브 샘플 사용)
- Skip-gram 이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향이 있다.

***

## FastText

- 2017년 페이스북에서 개발해 공개한 단어 임베딩 기법
- FastText는 각 단어를 문자 단위 n-Gram으로 표현한다. 이 밖의 내용은 Word2Vec과 같다. (네거티브 샘플링)
- FastText 도델의 강점은 *조사나 어미가 발달한 한국어* 에 좋은 성능을 낼수 있다.
- 오타나 미등록 단어에도 강건하다, 다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다.
- 한글은 자소 단위로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText을 시행할 수 있기 때문이다.
  - 한글 자소분해 예시 : 'ㄴㅏ-ㄴㅡㄴㅎㅏ'

***

## LSA (잠재 의미 분석)

- 단어 문서 행렬, TF-IDF행렬, 단어-문맥 행렬에 차원 숙소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우는 한편 행간에 숨어 있는 잠재 의미를 이끌어 내기 위한 방법론이다.
- ....

***

## GloVe (Global Word Vectors)

- 2014년 미국 스탠포드대학교연구팀에서 개발한 단어 임베딩 기법이다.
- Word2Vec, LSA 단점 극복
- '임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 잘 반영해보자'가 GloVe가 지향하는 핵심 목표다.

***

## Swival (Submatrix-Wise Vector Embedding Learner)

- 2016년 구글 연구팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다.
- ...

***

## 가중 임베딩

- 2016년 미국 프린스턴대학교 연구팀이 ICLR에 발표한 방법론이다.
- 문서 내 단어의 등장은 저자가 생각하는 주제에 의존한다고 가정한다. (주제 벡터)
