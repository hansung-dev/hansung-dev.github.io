---
category: NLP
path: '/stuff/:id'
title: '단어 수준 임베딩'
type: 'NLP'

layout: nil
---

# 단어 수준 임베딩
> 출처 : 한국어 임베딩 4장 단어 수준 임베딩

1. 예측 기반 모델 : NPLM, Word2Vec, FastText
2. 행렬 분해 기반 기법 : LSA, CloVe, Swivel
3. 가중 임베딩 : 단어 임베딩을 문장 수준으로 확장하는 방법

* * *

## NPLM (Neural Probabilistic Language Model)

- '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생
- NPLM은 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델이다.
- 예시 : '발', '없는', '말이', ? < 예측 ('천리')

* * *

## Word2Vec

- 2013년 구글 연구 팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델이다.
- Skip-Gram, CBOW 그리고 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안
  - CBOW : 주변에 있는 문맥 단어 들을 가지고 타깃 단어 하나를 맞추는 과정
  - Skip-gram : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정 (네거티브 샘플링, 서브 샘플 사용)
- Skip-gram 이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향이 있다.
- 
