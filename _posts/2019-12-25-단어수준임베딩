---
category: NLP
path: '/stuff/:id'
title: '단어 수준 임베딩'
type: 'NLP'

layout: nil
---

# 단어 수준 임베딩
> 출처 : 한국어 임베딩 4장 단어 수준 임베딩

1. 예측 기반 모델 : NPLM, Word2Vec, FastText
2. 행렬 분해 기반 기법 : LSA, CloVe, Swivel
3. 가중 임베딩 : 단어 임베딩을 문장 수준으로 확장하는 방법

* * *

## NPLM (Neural Probabilistic Language Model)

- '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생
- NPLM은 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델이다.
- 예시 : '발', '없는', '말이', ? < 예측 ('천리')

* * *

## Word2Vec

- 2013년 구글 연구 팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델이다.
- Skip-Gram, CBOW 그리고 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안
  - CBOW : 주변에 있는 문맥 단어 들을 가지고 타깃 단어 하나를 맞추는 과정
  - Skip-gram : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정 (네거티브 샘플링, 서브 샘플 사용)
- Skip-gram 이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향이 있다.

* * *

## FastText

- 2017년 페이스북에서 개발해 공개한 단어 임베딩 기법
- FastText는 각 단어를 문자 단위 n-Gram으로 표현한다. 이 밖의 내용은 Word2Vec과 같다. (네거티브 샘플링)
- FastText 도델의 강점은 *조사나 어미가 발달한 한국어*에 좋은 성능을 낼수 있다.
- 오타나 미등록 단어에도 강건하다, 다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다.
- 한글은 자소 단위로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText을 시행할 수 있기 때문이다.
  - 한글 자소분해 예시 : 'ㄴㅏ-ㄴㅡㄴㅎㅏ'

* * *
